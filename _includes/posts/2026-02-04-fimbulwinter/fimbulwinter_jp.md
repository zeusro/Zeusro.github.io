## 要約

本稿は形式論理と定義（ネットワーク効果、エントロピー増大の法則、反依存性、限界便益ゼロの「ゼロ」の哲学）から出発し、2014年から2025年までのアリババクラウド、Google Cloud、Azure、Cloudflare、テンセントクラウドなど主要パブリッククラウド・インフラの重大障害を体系的に整理し、各社の障害透明性を比較する。その上で、パブリッククラウドの「死因」を分析する：ソフトウェアのエントロピーと限界便益ゼロによりレガシーコード・アーキテクチャの治理が困難になること、障害時にネットワーク効果が連鎖反応を増幅すること、SLA賠償が政企の実際の損失と著しく乖離していること。著者は大型政企が単一パブリッククラウドへの「反依存」を優先すべきだとし、「小並行高可用システム」の構想を提示する——ストレージ冗長とトラフィック分散（例：マルチリージョン・マルチクラスタのDNS解決）により単一点リスクと影響半径を低減する。

![芬布尔之冬（フィンブルの冬）](/img/in-post/fimbulwinter/fimbulwinter.jpeg)

## 形式論理と定義

**ネットワーク効果**：製品・サービス・プラットフォームの利用者（または参加者）が増えるほど、各利用者にとっての価値が高まる。

**エントロピー増大の法則**：プロジェクトのコードベースは時間とともにスパゲッティ化する。

**反依存性**：単一のプログラミング言語・技術スタック・クラウドプラットフォームへの依存の逆。例：多言語混在プログラミング、マルチスタック選定、マルチクラウド戦略。反依存性により、業務は言語・オープンソース・パブリッククラウドに縛られず安定稼働できる。

**ゼロ**：限界便益がゼロのリファクタリングは誰もやりたがらない。

## AWSの障害

| 時期              | 地域 / 範囲                | 継続時間          | 主な影響と結果                                                                             | 公式 / 主な原因                                    | 備考 / 業界評価                                          |
|-------------------|----------------------------|-------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------|----------------------------------------------------------|
| 2014-06-13       | us-east-1 (北バージニア)   | 未詳              | Amazon SimpleDB の重要コンポーネントに影響、一部ユーザーがデータベース操作不可。           | 電源関連の問題                                     | 小規模、迅速回復；電源冗長性の不足が露呈。               |
| 2014-08-07       | EU West (アイルランド)     | 未詳              | EC2、EBS、RDS に影響；欧州の一部ユーザーがインスタンス・ストレージ・DB にアクセス不可。   | 内部システムイベント（詳細非公開）                 | 地域限定；初期の地域分離の限界を示した。                 |
| 2014-11-26       | グローバル (CloudFront DNS)| 約2時間           | DNSサーバーダウン、CDNに影響；多数のウェブサイト・クラウドサービスが名前解決不能。         | DNSサーバー障害                                    | 短時間だが広範囲；世界最大クラウドのDNS脆弱性が露呈。     |
| 2015-09-20       | us-east-1                  | 数時間            | DynamoDB障害→内部通信途絶；Netflix、Reddit、IMDb、Amazon系サービスに広範な影響。           | 内部サービスの連鎖反応                              | 深刻なインシデント；数百万ユーザーに影響；内部依存の危険性を警告。 |
| 2017-02-28       | us-east-1                  | 4〜5時間          | S3制御平面停止；Slack、Trello、GitHub Pages、Quoraなど多数のサイトで画像・ファイル消失。   | エンジニアの誤操作（重要な設定の誤削除）           | 史上最も有名な障害の一つ；「史上最も高価なタイポ」。     |
| 2020-11-25       | us-east-1                  | 数時間            | KinesisとCognito障害；Roku、Adobe、Flickrなどに影響；ストリーミング・認証サービス停止。   | キャパシティ更新の問題                              | パンデミック中の発生；キャパシティ計画の難しさを浮き彫りに。 |
| 2022-12-05       | us-east-2 (オハイオ)       | 約40分            | 可用性ゾーン障害；地域内の複数サービスに影響。                                             | 詳細非公開（ネットワーク/電源の可能性）             | 短時間だがマルチAZの重要性を強調。                       |
| 2023-06-13       | us-east-1                  | 未詳              | Lambdaサービス障害；Boston Globe、NYC地下鉄、AP通信などに大きな影響。                      | 詳細非公開                                         | サーバーレスコンピューティングの大規模時の脆弱性を露呈。   |
| 2024-07-30       | us-east-1                  | 未詳              | Kinesis Data Streams障害；リアルタイムデータ処理アプリケーションに連鎖影響。               | 詳細非公開                                         | ストリーム処理依存のリスクを浮き彫りに。                 |
| 2025-02          | eu-north-1 (ストックホルム) | 未詳              | 1つのAZで重大なネットワーク障害；欧州のコアサービスに影響。                               | 大規模ネットワーク障害                              | 地域限定；欧州インフラ拡張の課題を反映。                 |
| 2025-10-19/20    | us-east-1                  | 15時間以上        | DynamoDB APIエンドポイント障害（DNS障害）；Slack、Reddit、Roblox、Fortnite、Coinbase、Venmo、Duolingo、Canva、PSN、銀行、航空会社（Delta/United遅延）、Amazon系サービスに影響；3500社以上、60カ国以上、1700万件超の報告。 | DNS解決障害による連鎖反応                           | 2025年最大のクラウド障害；「クラウドの脆弱性が最も露呈した事件」；マルチクラウド戦略が強く推奨される。 |

**最も深刻な障害の分析**  
2014〜2025年で最も深刻だったのは2015年9月（DynamoDB）、2017年2月（S3）、特に2025年10月（DynamoDB DNS）の3件。すべてus-east-1で発生。  
深刻度順：  
- 2015年：エンタメ・SNSに大きな打撃だが、主に消費者向け。  
- 2017年：生産性ツール・開発者に甚大な影響；人的ミスによる経済損失が極めて大きい。  
- 2025年：最長時間（15時間超）、最も広範（金融、ゲーム、教育、航空、重要インフラ）、ユーザー影響最大 — 近年最大の単一クラウド障害と評価。  
共通教訓：us-east-1への過度な集中、コントロールプレーンの脆弱性、連鎖障害。業界はマルチリージョン、マルチクラウド、ハイブリッドアーキテクチャの採用を強く推奨している。


## アリババクラウドの障害

2014年から2025年まで、アリババクラウドが公表・公式開示した**影響範囲が広く、継続時間が長く、「重大事故」と広く呼ばれた**障害は（体量に比べ）特に多くはなく、全体のSLAは業界トップクラスである。ただしそのうち数件は社会的影響と業界議論を引き起こした。以下は**重大級**の障害の時系列リスト（主に公開情報・公式発表・メディア・コミュニティの振り返りに基づく）：

| 時期 | 地域/範囲 | 継続時間 | 主な影響と結果 | 公式/主流の原因 | 備考/業界評価 |
|------|------------|----------|----------------|-----------------|----------------|
| 2018年6月 | 一部地域（詳細非公開） | 約30分 | 一部クラウド製品異常、影響は限定的 | 詳細非公開 | 当時メディアにより「重大技術障害」と呼ばれた |
| 2019年3月3日 | 華北2（北京）AZ-C | 数時間 | ECSディスク障害多発、多数サイト/アプリ停止 | ディスク障害 | 影響大、アリババクラウドはSLAに基づき賠償 |
| 2022年12月18日 | 香港リージョンAZ-C | 約15.5時間 | 香港域ほぼ全域停止、マカオの重要機関（金融管理局、銀河、蓮花衛視等）サイト不通、OKX影響 | 冷却設備故障→連鎖反応で大規模ダウン | 「アリババクラウド史上最悪の醜聞の一つ」と広く称された |
| 2023年11月12日 | **全世界域・全サービス** | 約3時間16分 | コンソール・API・MQ・マイクロサービス・監視・ML等ほぼ全線異常；淘宝・釘釘・閑魚・餓了么・阿里雲盤等一斉「クラッシュ」 | 基盤コアコンポーネント（認証/メタデータ/コントロールプレーン）障害 | **アリババクラウド史上最悪・影響最広**と公認；「エピック」「業界前例なし」 |
| 2023年11月27日 | 一部サーバー | 約2時間 | サーバーアクセス異常 | 詳細非公開 | 11・12事故の半月後、信頼への疑問再燃 |
| 2024年7月2日 | 一部地域/サービス | 数時間 | コンソール及び一部サービス異常 | 詳細な振り返りなし | 中規模、以前の数件より影響小 |
| 2025年（日付不明） | グローバル（DNS関連疑い） | 約6時間 | DNSハイジャックによりグローバルサービス異常 | DNSハイジャック関連 | 2025年パブリッククラウド障害まとめより、詳細要確認 |

### 重要な注記とトレンド

1. **2014–2018年初期**：この期間のアリババクラウドの重大P0級障害の公表は極めて少なく、局所・小規模が中心。当時は体量が今よりはるかに小さく、影響も限定的だった。

2. **最悪の二件**：
   - **2022.12 香港15.5時間** → 単一地域最長障害、港澳の重要インフラに悪影響。
   - **2023.11.12 グローバル3時間超** → コントロールプレーン/グローバルサービス全面停止；「全地域×全サービス」同時故障として稀とされ、「マルチアクティブ・マルチセンター・Nナイン」神話を打ち砕いた。

3. **2024–2025**：既知の情報では、2022–2023より頻度・深刻度は低下しているが、中〜大規模事象は依然発生（特に2025年DNSハイジャック系は影響範囲が広い）。

4. **アリババクラウドの対応**：重大障害の多くで詳細な振り返りを公表（特に2022香港・2023グローバル）；SLA賠償（通常は障害時間に応じた倍率のバウチャー）；幹部/公式の謝罪。

全体として、2014–2025年の12年間でアリババクラウドが「重大事故」と呼べるのはおおよそ**5〜7件**、その中で**2023年11月12日**と**2022年12月香港**が影響力・深刻度で最も高いと公認されている。


## グーグルクラウドの障害

Google Cloud Platform（GCP）は2014年から2025年まで、全体のSLAは比較的良好で、**グローバル**または**複数コアサービス**に真に影響した**重大事故**は（AWS・Azureに比べグローバル災害級は少ないが）発生時には多くのサードパーティアプリ（Snapchat、Spotify、Discord、Cloudflare依存サービス等）を巻き込み、社会的影響が大きい。

以下は**深刻で影響範囲の広い**GCP重大障害の時系列リスト（公開Status Dashboard・メディア・Wikipedia・業界振り返り等に基づく）：

| 時期 | 地域/範囲 | 継続時間 | 主な影響と結果 | 主流の原因 | 備考/業界評価 |
|------|------------|----------|----------------|------------|----------------|
| 2015年8月 | 欧州（ベルギーGhlin） | 数時間 | Compute Engine 読み書きエラー率極高、一部データ損失 | 落雷でデータセンター一部機器損傷 | グーグルが**データ損失**を認めた稀有な例、影響は限定的顧客群 |
| 2018年7月 | グローバル（多地域） | 数時間 | GCP多サービス異常、Snapchat・Spotify等サードパーティが広範囲でログイン/利用不可 | ネットワーク輻輳＋内部ルーティング | 当時広く報道、サードパーティ影響が顕著 |
| 2019年6月2日 | 米東部＋グローバル波及 | 約4–5時間 | YouTube・Gmail・G Suite広範囲不可用、Snapchat・Discord・Vimeo等ログイン失敗 | 米東部ネット輻輳＋カスケード | 影響範囲広、SNSで話題 |
| 2020年12月14日 | グローバル | 約1時間 | Gmail・YouTube・Google Home・Nest・Pokémon GO等、認証依存サービスがほぼ全滅 | 認証システム（IAM相当）グローバル障害 | コンシューマサービス一斉「クラッシュ」、最大級の一つ |
| 2022年8月 | 米アイオワ州データセンター | 局所 | 電気火災（負傷3名）、一部サービス波及、グローバルではない | データセンター電気事故による火災 | 物理施設事故、純粋なソフト/アーキテクチャ問題ではない |
| 2023年4月 | 欧州（パリ等） | 数時間 | 多地域ネット＋サービス中断 | 洪水＋データセンター＋ネット問題 | 気象要因、影響は中程度 |
| 2024年10月23日 | 欧州（フランクフルト europe-west3） | 約半日（12時間超） | 当該地域ほぼ全サービス不可、欧州顧客多数に影響 | 詳細非公開（コントロールプレーン/ネット疑い） | 単一地域最長障害の一つ |
| **2025年6月12日** | **グローバル**（40超地域） | 約2.5–3時間 | **70超GCPサービス**異常、IAM認証崩壊でAPIリクエスト全面失敗；Spotify・Discord・Twitch・Cloudflare・Fitbit・Gmail・Drive・YouTube等広範囲停止 | **Service Control**（API認証コア）の自動更新が深刻バグ導入→クラッシュループ→グローバル過負荷 | **2020年以降GCP最悪のグローバル障害**と公認 |
| 2025年7月18日 | us-east1 | 約2時間 | 複数製品エラー率上昇 | 詳細非公開 | 中規模、回復は比較的速い |


## Azureの障害

Microsoft Azureは2014年から2025年まで、**影響範囲が広く、継続時間が長く、社会的注目が高い重大事故**の数は（初期の小規模時の頻繁な小障害に比べ）特に多くはないが、数件はグローバルまたは多サービス級の深刻な影響を与え、**Microsoft 365・Teams・Xbox・Outlook**等のコンシューマ/エンタープライズ製品が絡むと伝播効果が非常に大きい。

以下は**重大級**Azure障害の時系列リスト（Azure公式Status History・Post Incident Review・メディア・Wikipedia・業界振り返り等の公開情報に基づく）：

| 時期 | 地域/範囲 | 継続時間 | 主な影響と結果 | 主流の原因 | 備考/業界評価 |
|------|------------|----------|----------------|------------|----------------|
| 2014年8月14–18日 | US Central・US East・US East 2・Europe North | 複数日・1回数時間 | Cloud Services・SQL Database・VM・Websites・HDInsight・Mobile Services・Service Bus等広範囲不可用 | 複数ネット/ストレージ問題 | 2014年最も集中した一波、当時Azureはまだ若い |
| **2014年11月18–19日** | **多地域**（米・欧・アジア一部） | 約11時間 | Azure Storageを中心にVM・Websites・Visual Studio Online・Xbox Live・MSN・Search等20超サービス中断 | ストレージ性能最適化の設定変更→Blobフロント無限ループ | **Azure初期最悪の一件**、公式詳細RCA、顧客賠償 |
| 2016年9月15日 | **グローバル** | 数時間 | DNS解決広範囲障害、Azure DNS依存サービス多数に影響 | グローバルDNS問題 | DNS単点リスク露呈 |
| 2018年6月20日 | 北米複数データセンター | 数時間–1日超 | 冷却系故障（落雷＋サージ保護不足）で多サービス中断 | 物理施設（落雷連鎖） | 稀なハードウェア/インフラ級障害 |
| 2018年9月4日 | **多地域** | 25時間超（一部サービス3日） | 複数コアサービス長時間不可用 | 冷却（落雷＋サージ） | 回復最長の一つ |
| 2023年1月23日 | **グローバル**（コアネット影響） | 約3時間 | Microsoft 365（Teams・Outlook・Exchange）、一部Azureサービス中断 | WAN問題 | M365一斉「クラッシュ」、影響巨大 |
| 2024年7月18日 | US Central | 約半日 | VM等の管理操作失敗、顧客がマネージドサービスにアクセス不可 | アクセス制御エラー＋インフラ障害 | 翌日CrowdStrike全球ブルースクリーンに近接するが独立 |
| 2025年1月8–9日 | East US 2等 | 数時間 | Azure Databricks・Synapse・Functions・App Service・VM等ネット中断 | ネットワークコンポーネント問題 | 2025年序盤で顕著な一件 |
| **2025年10月29日** | **グローバル** | 約8時間 | Azure Front Doorを中心にMicrosoft 365・Outlook・Teams・Xbox Live・Minecraft・Copilot広範囲停止；Alaska Airlines・Heathrow・Costco・Starbucks等波及 | **Azure Front Door設定変更**＋保護機構のバグで設定不整合がグローバル伝播 | **2025年Azure最悪**、Downdetector3万超報告、同月AWS障害に類似 |
| 2025年11月5–6日 | West Europe (AZ01) | 約9–10時間 | VM・PostgreSQL/MySQL Flexible Server・AKS・Storage・Service Bus等多サービス劣化/中断 | データセンター熱イベント | リージョン級で深刻な障害 |

### 観察とトレンド（2014–2025）

- **2014年**：Azureは急拡大期；**設定変更**と**ストレージ層**問題が頻発、障害が最も集中した年（特に11月は古典的ケース）。
- **2015–2019**：障害頻度は低下、依然**単一地域**または**インフラ**（冷却・落雷・DNS）が中心、影響は比較的抑制。
- **2020–2023**：重大グローバル障害は少なめ、**ネット**または**M365のAzure依存**による間接影響（例：2023年1月）が中心。
- **2024–2025**：コントロールプレーン/エッジ（**Azure Front Door**等）が新たな痛み；2025年10月29日は近年Azure最悪の**グローバル中断**とされ、アリババ2023年11月やGCP 2025年6月に匹敵。
- **典型**：重大事故のたびに詳細な**Post Incident Review (PIR)** を公表、透明性が高い；**設定変更**・**コントロールプレーン**・**ネット**が原因となることが多く（単なるハード障害ではない）；M365・Xbox・Teamsが落ちるとサードパーティ伝播が極めて大きい；SLA賠償（クレジット）、顧客は事業継続性を最重視。

全体として、Azureは2014–2025年の12年間で**重大級（グローバル/多サービス長時間中断）**はおおよそ**8–10件**、深刻度・頻度はAWS・GCPと同水準で、**設定ミスによるカスケード**が繰り返し現れるパターン。


## Cloudflareの障害

Cloudflareは世界最大級のCDN・セキュリティ・DNS・エッジプロバイダーの一つとして、2014年から2025年まで**影響範囲が広くインターネット大規模麻痺を引き起こした重大事故**は特に多くはないが、一度起きると**数百万〜数億ユーザー**に波及し（Cloudflareは全世界の約20–25%のウェブトラフィックを担う）、影響範囲が極めて広い。

特徴は**回復が比較的速い**（多くは1–4時間で緩和）一方で**伝播が極端**——コアプロキシ・DNS・セキュリティコンポーネントが落ちると、X・ChatGPT・Shopify・Discord・Spotify・AWS一部等のトップサイトが同時に5xxやアクセス不可になる。

以下は**重大級**Cloudflare障害の時系列リスト（公式ブログ・status.cloudflare.com履歴・メディア・Wikipedia・業界振り返り、グローバル/コアトラフィック中断に焦点）：

| 時期 | 範囲 | 継続時間 | 主な影響と結果 | 主流の原因 | 備考/業界評価 |
|------|------|----------|----------------|------------|----------------|
| **2019年7月2日** | **グローバル** | 約1–2時間 | 多数サイトで502/503/504、インターネットの広域がアクセス不可 | ソフトウェアデプロイが深刻バグを導入→プロキシ層クラッシュ | **Cloudflare史上最悪**と公認、公式詳細振り返り |
| 2020年複数回 | 一部地域/コントロールプレーン | 数時間 | ダッシュボード・分析・一部API不可；コアプロキシはほぼ安定 | コントロールプレーン問題 | 開発者への影響大、一般ユーザーは感知小 |
| 2022年6月 | **複数データセンター**（19） | 約1.5時間 | コアプロキシ中断、多数サイトアクセス不可 | ネットワーク設定エラー | 中規模、回復は速い |
| 2025年3月21日 | **グローバル** | 約1時間7分 | ストレージ読み書き深刻障害、ストレージ/キャッシュ依存サービス多数に影響 | KV/ストレージ層書き込み失敗＋一部読み異常 | 2025年序盤で顕著 |
| 2025年6月12日 | **グローバル**（一部機能） | 数時間 | 一部機能/サービス不可、コアトラフィックはほぼ正常 | 特定モジュールデプロイ | コアトラフィック中断ではなく影響は限定的 |
| 2025年7月14日 | **グローバル**（1.1.1.1 DNS） | 約62分 | パブリックDNSリゾルバ（1.1.1.1）完全不可、多数ユーザーがインターネット接続不可 | 設定エラー→BGPルート撤回→DNSプレフィックスがグローバルルーティングテーブルから消失 | **1.1.1.1依存ユーザーに極めて深刻**、「断網級」 |
| 2025年10月頃 | 一部サービス | 数十分 | DNS解決の短時間中断 | DNS関連設定 | 中規模 |
| **2025年11月18日** | **グローバル** | 約4–5時間（ピークはより長い） | **大規模インターネット中断**：X（Twitter）・ChatGPT・Shopify・Spotify・Letterboxd・Indeed・Canva・Uber・DoorDash・Truth Social・League of Legends等多数トップサービス麻痺；約20%ウェブトラフィック、Alexa上位1万サイトの1/3が影響 | Bot Managementルールファイル異常肥大（DB権限変更でファイルサイズ倍増）→全网伝播→プロキシクラッシュ | **2025年最悪**、2019年以来最悪のグローバルトラフィック中断 |
| 2025年12月5日 | **グローバル** | 数時間 | 再び広範囲5xx；Shopify・Zoom・Vinted・Fortnite・Square・Just Eat・Canva・Vimeo・AWS一部・Deliveroo等影響 | 完全な公式根因なし（設定/伝播類疑い） | **11月18日からわずか17日**、連続二回の重大事故で強い批判 |

### 観察とトレンド（2014–2025）

- **2014–2018年初期**：Cloudflareは急成長；公表された**グローバル重大**障害は少なく、局所・地域・機能問題が中心；当時インターネットのCloudflare依存度は今より低い。
- **2019年7月**：Cloudflare史上の古典的「ブラックスワン」；その後6年以上、同規模の**コアプロキシ全局中断**はなし。
- **2025年は異常な年**：少なくとも**3–4回**の影響の大きいグローバル/準グローバル事象（特に11月18日と12月5日の連続）。**11月18日**は**2019年以来最悪**と公認。12月5日の再発で、変更管理・ロールバック・「fail small」の実行が疑問視された。
- **典型**：深刻な障害の多くが**設定変更**・**ルール/伝播**・**コントロールプレーン**または**DNS/BGP**に関連；blog.cloudflare.comで毎回詳細なpost-mortem（根因・タイムライン・改善策）を公開、透明性が高い；回復は通常速い（ロールバック＋伝播停止）が影響は極めて広い（Anycast＋チャレンジ機構）；AWS/Azure/Googleのような明確なSLAクレジット賠償はないが、詳細説明と改善コミットを提供。

全体として、Cloudflareは2014–2025年の12年間で**重大級（グローバルコアトラフィック長時間中断）**はおおよそ**5–7件**、**2019年7月**と**2025年11月18日**が二つのピーク。2025年は障害頻度が明らかに上昇し、「インターネットインフラ集中化リスク」の議論が再燃した。


## テンセントクラウドの障害

テンセントクラウドは2014年から2025年まで、中国第二のパブリッククラウド（アリババに次ぐ）として、国内クラウドベンダーの中では安定性は中〜上で、**影響が極めて広く、継続時間が長く、社会的注目が高いグローバル/多地域重大事故**は相対的に少ないが、コントロールプレーン（コンソール/API）やコアストレージに問題が起きると、影響は多数の企業・開発者に急速に拡大する。

テンセントクラウドの**ステータスページ**（https://status.cloud.tencent.com/history）の公開透明性は比較的低く、履歴は通常直近1年のみで、中〜大規模障害の多くは一覧に載らず、公式微信・技術ブログ・メディア・コミュニティで全体像を補う。

以下は**深刻で影響範囲の広い**テンセントクラウド障害の時系列リスト（公式振り返り・メディア・知乎/微博/開発者コミュニティ等の公開情報に基づく）：

| 時期 | 地域/範囲 | 継続時間 | 主な影響と結果 | 主流の原因 | 備考/業界評価 |
|------|------------|----------|----------------|------------|----------------|
| 2014年11月2日 | **全国**（コントロールプレーン＋一部サービス） | 約6分 | テンセントクラウド公式サイト遅延・画像読込失敗・コンソール異常、一部ユーザーが正常利用不可 | 詳細非公開（ネット/負荷疑い） | 初期小規模期、影響は限定的だが当時メディアで広く報道 |
| 2018年8月 | 一部ユーザー/クラウドディスク | 不明（単一ユーザーで数時間〜恒久） | 複数ユーザーのクラウドサーバーディスクデータ**ゼロ/消失**、千万単位の損失 | ディスク静默エラー＋データ移行時の検証/レプリカ機構不全 | **テンセントクラウド史上最悪の「データ消失」**、クラウド信頼危機、公式詳細振り返り |
| 2023年（散発） | 一部地域/サービス | 数十分–数時間 | 散発的なコンソール/API異常、ストレージジッター | 公開の詳細振り返りなし | アリババ2023年11月グローバルと比べテンセントは比較的安定 |
| **2024年4月8日** | **全球17地域** | 約74–87分 | コンソール完全ログイン不可、クラウドAPI全面504 Gateway Timeout；CVM/RDS等インスタンスは稼働するが管理/更新/スケール不可；1957件の顧客障害報告 | クラウドAPI新バージョンの後方互換不足＋設定データのグレーリリース機構欠如→一斉リリースでグローバル伝播 | **テンセントクラウド近年最悪**、「全球大障害」「コントロールプレーン崩壊」と広く呼ばれ、アリババ2023年11月スタイルに類似 |
| 2025年10月15日 | 多地域 | 約数十分–1時間 | 弹性伸缩（Auto Scaling）等サービス異常 | 詳細非公開 | ステータスページより、中規模 |
| 2025年10月17日 | 広州地域 | 約1時間超 | 智能数智人関連サービス異常 | 詳細非公開 | リージョン級、特定AI/デジタルヒューマン製品 |

### 観察とトレンド（2014–2025）

- **初期（2014–2018）**：障害は**ストレージ層のデータ消失**や**短時間アクセス異常**が典型；2018年「データ消失」が企業信頼を最も痛撃。
- **2019–2023**：テンセントクラウドの障害頻度・深刻度は明らかに低下、全国/グローバル級は稀、安定性は同期のアリババより良好（特に2023年アリババ11・12エピック障害時はテンセントは安定）。
- **2024–2025**：2024年4月8日が転換点；この**コントロールプレーン全局障害**で多くの人がテンセントクラウドの「変更安全」と「グレーリリース」能力を再検討。2025年は中規模が数回あるが、2024年4月やアリババ/グーグルクラウドのような「全サービス麻痺」級は見られない。
- **典型**：**コントロールプレーン/API**が近年の最大の痛み（2024年4月が代表）；**ストレージ/データ消失**が企業への打撃が最大（2018年事例）；公式振り返りは比較的タイムリー（重大時は微信・技術コミュニティで詳細説明）；AWS/Azure/Googleのような厳格なSLAクレジット賠償はないが、バウチャー/補償を提供；障害の伝播はアリババ・Cloudflareほど劇的ではない（顧客構成が企業/ゲーム/動画寄りで、コンシューマインターネット依存が相対的に低いため）。

全体として、テンセントクラウドは2014–2025年の12年間で**重大級（全国/グローバルコントロールプレーン長時間不可または深刻なデータ消失）**と呼べるのはおおよそ**3–5件**、**2018年データ消失**と**2024年4月8日グローバルコントロールプレーン障害**が影響力・議論度で最も高い二件とされている。


## 障害の透明性

障害の透明性という点では、アリババクラウドとテンセントクラウドは比較的弱い。

アリババクラウドのステータスボード（https://status.aliyun.com/#/?region=cn-shanghai）とテンセントクラウドのステータスボード（https://status.cloud.tencent.com/history）は、過去1年間のイベントしか表示しない。

Azure（https://azure.status.microsoft/en-us/status/history/）は5年分を保持。Cloudflare（https://www.cloudflarestatus.com/history?page=17）が最も詳細で透明で、pageで数年分まで遡って閲覧できる。


## パブリッククラウドの「死」

若い頃、私は他人のコードを読んでリファクタリングするのが好きだった。Java Boyに教えられるまで：彼のメモリリークを直したのに、彼が感じたのは深い実存的不安だった。彼は怒りで自分の無能を隠し、「Kubernetesプラットフォームの安定維持」の責任を私に押し付けた。

私も彼の尻拭いを喜んでやっていたわけではない。ただ、頻発する`OOM kill`イベントアラートが煩わしかっただけだ。その話から、ゼロの哲学を悟った：**限界便益がゼロのリファクタリングは誰もやりたがらない。**

ゆえにソフトウェア業界の**エントロピー増大の法則**が導かれる：プロジェクトのコードベースは時間とともにスパゲッティ化する。

「これがパブリッククラウドの死と何の関係がある？」と聞く人がいた。

実は大いに関係がある。「工業的クトゥルフ」は企業に時間とともに増える利益を要求する。だからパブリッククラウドベンダーとその現場社員は「新しい成長物語」を語らなければならない——2026年なら、その物語はAIだ。

古いコードはそのまま腐る。変更しても限界便益はない。

やがて誰もが同じ合意に至る：他人のコードには触れず、スパゲッティ化に任せる。コードもアーキテクチャも同じ。ネットワークトポロジーが純粋な相互依存の網の目になっても、障害時はみんな使えず、ダウン責任が平等に分散すれば、自分に責任はない。

皮肉なことに、在職中に障害ゼロで毎日「仕事中に寝ている」ように見える運用エンジニアは、無能だと見なされる。何もしていないように見えるから。しかし直感に反して、その人は企業のマスコットとして祀るべきだ。彼が以前どれだけ業務安定のために努力したか、あるいは単に運が良かっただけなのか、誰にもわからない。大廟に配享するに値する。

究極の運用は、運用しないことである。**運用の報酬とキャリアリスクは完全に不均衡**だからだ。上司は「使っていなさそうな設定」を消しても今月の手当を増やしてくれない。だが「使っているデータベース設定」を消せば、顧客に罵倒される。

パブリッククラウド企業は忘れている。ネットワーク効果は彼らを押し上げることも、叩き落とすこともできる。顧客が増えれば、単一のダウンによるネットワークの連鎖反応は大きくなる。2025年12月4日夜21:00–23:37頃の支付宝障害のように——これは阿里系2025年の第六の重大障害だった。

ユーザー規模が億単位になると、一秒ごとに誰かが支払っている。それでも業務はソフトに新機能を追加し続けを求め、やがてシステムは耐えられなくなる。

しかもパブリッククラウドの賠償は完全に不均衡だ。支付宝のようなケースではせいぜい「多退少補」、少なく徴収した分を福利として返す程度。だが政府・企業の損失は到底算定できない。業務がパブリッククラウドに載っていて、クラウドがダウンしたら、業務側はクラウドにどう自分の損失を説明するのか？

「うちのシステムは一日数百億取引している。数億賠償してくれる？」

顧客の損失は定量化できない。だからアリババクラウドは通常、バウチャーを少し渡すだけだ。それは焼け石に水。失われた時間、実際の業務影響、その時間価値を誰もはっきり言えない。

ネットワーク効果はパブリッククラウドに指数関数的な収益成長をもたらした。しかし大型の政府・企業ユーザーにとっては、パブリッククラウドへの**反依存**を日程に載せるべきだ。自らの運命を単一のクラウドベンダーに縛れば、突発リスクには対応できない。


## 小並行・高可用システム

その上で、私は「小並行・高可用システム」を提案する。**ストレージの冗長**により、業務の**高可用性**を実現する。

トラフィックが冗長な情報システムに平準に分散されれば、集中型トラフィックによる単一クラスタのトラフィックピークを避け、問題の拡散半径を縮小できる。

最も単純な例。DNS解決の際、広東のDNSを華南地域のアリババ・テンセントKubernetesクラスタに向ける。各クラスタは互いに依存せず、内部で完全な情報業務システムを稼働させる。最悪の場合、パブリッククラウド自体に問題が起きても、不可用は50%までに抑えられる。

両方同時に不可用になる確率は極めて小さい。


## Farewell, Public Cloud. See you on the Yangtze River.



## 参考リンク

【1】  
2025年11月18日 Cloudflare サービス中断  
https://blog.cloudflare.com/zh-cn/18-november-2025-outage/

【2】  
AWS障害から見るDNSの見えざる杀伤力：DeepFlowが混乱の中でいかに素早く根因を特定したか  
https://my.oschina.net/u/3681970/blog/18697034

【3】  
2023-11-12 アリババクラウド障害の振り返りと分析  
https://github.tiankonguse.com/blog/2023/11/29/aliyun-break.html
