
## 形式逻辑和定义

**网络效应**：一个产品/服务/平台的使用者（或参与者）越多，它对每一个使用者的价值就越大。

**熵增定律**：项目代码库会随着时间推移而变成一座屎山。

**反依赖性**：对单一编程语言/技术栈/云平台的反向依赖。比如多语言混合编程，多技术栈选型，多云策略。反依赖性让业务可以脱离语言/开源项目/公有云平稳运行。

**0**：边际收益为0的代码重构没人愿意做。


## 阿里云的故障

阿里云自2014年至2025年期间，公开报道和官方披露的**真正影响面广、持续时间长、被广泛称为“重大事故”**的故障并不算特别多（相比体量来说，整体SLA表现仍属行业前列），但其中几次确实造成了较大社会影响和行业讨论。下面按时间顺序列出公认的几次**重大级别故障**（主要基于公开信息、官方公告、媒体和社区复盘）：

| 时间              | 地域/范围                  | 持续时长          | 主要影响范围与后果                                                                 | 官方/主流原因说明                          | 备注 / 行业评价                              |
|-------------------|----------------------------|-------------------|------------------------------------------------------------------------------------|---------------------------------------------|---------------------------------------------|
| 2018年6月         | 部分地域（具体未公开细节） | 约30分钟          | 部分云产品出现异常，影响范围相对有限                                              | 未详细公开                                  | 当时被部分媒体称为“重大技术故障”             |
| 2019年3月3日      | 华北2（北京）可用区C       | 数小时            | 大量ECS实例磁盘故障，众多网站/App瘫痪                                              | 磁盘故障                                    | 当时影响较大，阿里云按SLA赔付               |
| 2022年12月18日    | 香港Region 可用区C         | 约15.5小时        | 香港区几乎全地域服务中断，澳门多家关键机构网站（金融管理局、银河、莲花卫视等）不可用，OKX交易受影响 | 制冷设备故障 → 连锁反应导致大面积宕机      | 被广泛称为“阿里云发展史上最严重丑闻之一”     |
| 2023年11月12日    | **全球所有地域、所有服务** | 约3小时16分钟     | 控制台、API、MQ、微服务、监控、机器学习等几乎全线产品异常；淘宝、钉钉、闲鱼、饿了么、阿里云盘等集体“崩” | 底层核心组件（鉴权/元数据/控制面组件）故障 | **公认阿里云史上最严重、影响范围最广的一次**，被称为“史诗级”“行业闻所未闻” |
| 2023年11月27日    | 部分服务器                 | 近2小时           | 服务器访问异常                                                                     | 未详细披露                                  | 距离11·12事故仅半个月，再次引发信任质疑     |
| 2024年7月2日      | 部分地域/服务              | 数小时            | 控制台及部分服务异常                                                               | 未见详细复盘                                | 属于中型故障，影响面小于前几次               |
| 2025年（具体日期不详） | 全球范围（疑似域名相关）   | 约6小时           | 域名劫持导致全球服务异常                                                           | 域名劫持相关                                | 来自2025年公有云故障汇总报告，细节待确认     |

### 重要说明与趋势观察

1. **2014–2018早期**：这个阶段阿里云对外公开的重大P0级故障记录极少，更多是局部、小范围问题。那时体量远小于现在，故障影响面也较小。

2. **最严重的两次**：
   - **2022.12香港区15.5小时** → 单地域最长时间故障，对港澳地区关键基础设施影响恶劣。
   - **2023.11.12全球3小时+** → 控制面/全局服务全部瘫痪，被公认为云计算历史上罕见的“全地域×全服务”同时故障，打破了阿里云长期标榜的“多活、多中心、N个9”神话。

3. **2024–2025**：从已知信息看，故障频次和严重程度相比2022–2023有所下降，但仍然偶有中大型事件（尤其是2025年的域名劫持类事件影响面较广）。

4. **阿里云的处理特点**：
   - 大部分重大故障后都会发详细复盘说明（尤其是2022香港、2023全球两次）
   - 按SLA赔付（通常是故障时长对应倍数的代金券）
   - 高管/官方会公开致歉

总体而言，阿里云在2014–2025这十二年间真正称得上“重大事故”的次数大约在**5–7次**左右，其中**2023年11月12日**和**2022年12月香港**是公认的影响力和严重程度最高的两起。


## 谷歌云的故障

谷歌云（Google Cloud Platform，简称GCP）从2014年到2025年这段时间内，整体SLA表现相对较好，真正影响**全球范围**或**多个核心服务**的**重大事故**并不算特别频繁（比AWS和Azure略少一些全局性灾难级事件）。但一旦出问题，通常会波及大量第三方应用（如Snapchat、Spotify、Discord、Cloudflare依赖的服务等），社会影响较大。

下面按时间顺序列出公认的**比较严重的、影响面广**的GCP重大故障（基于公开Status Dashboard、媒体报道、Wikipedia、行业复盘等信息）：

| 时间                | 地域/范围                  | 持续时长          | 主要影响范围与后果                                                                 | 主流原因说明                                      | 备注 / 行业评价                                      |
|---------------------|----------------------------|-------------------|------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------|
| 2015年8月           | 欧洲（比利时Ghlin）        | 数小时            | Compute Engine 读写错误率极高，少量数据丢失                                       | 雷击导致数据中心部分设备损坏                      | 谷歌罕见承认**数据丢失**，影响较小客户群            |
| 2018年7月           | 全球（多地域）             | 数小时            | GCP多服务异常，Snapchat、Spotify等第三方应用大面积无法登录/使用                   | 网络拥塞 + 内部路由问题                           | 当时被广泛报道，第三方影响很明显                    |
| 2019年6月2日        | 美国东部 + 全球波及        | 约4–5小时         | YouTube、Gmail、G Suite大面积不可用，Snapchat、Discord、Vimeo等登录失败           | 美国东部网络拥塞 + 级联故障                       | 影响范围广，社交媒体热议                            |
| 2020年12月14日      | 全球                       | 约1小时           | Gmail、YouTube、Google Home、Nest、Pokémon GO等几乎所有依赖身份验证的服务瘫痪     | 身份认证系统（类似IAM）全局故障                   | 谷歌消费级服务集体“崩”，影响最大的一次之一         |
| 2022年8月           | 美国爱荷华州数据中心       | 局部影响          | 电气火灾（3人受伤），部分服务受波及，但非全局                                     | 数据中心电气事故引发火灾                          | 更多是物理设施事故，非纯软件/架构问题              |
| 2023年4月           | 欧洲（巴黎等）             | 数小时            | 多地域网络 + 服务中断，部分客户受影响                                             | 洪水 + 数据中心问题 + 网络故障组合               | 天气因素导致，影响面中等                            |
| 2024年10月23日      | 欧洲（德国法兰克福 europe-west3） | 约半天（12+小时） | 该地域几乎全服务不可用，影响大量欧洲客户                                          | 未详细公开（疑似控制面/网络问题）                 | 单地域最长时长故障之一                              |
| 2025年6月12日       | **全球**（多地域，40+个）  | 约2.5–3小时       | **70+项GCP服务**异常，IAM身份授权系统崩溃，导致API请求全面失败；Spotify、Discord、Twitch、Cloudflare、Fitbit、Gmail、Drive、YouTube等大面积瘫痪 | **Service Control**（API鉴权核心组件）自动化更新引入严重bug → 崩溃循环 → 全局过载 | **公认2020年后GCP最严重的一次全局故障**，影响互联网大片区域 |
| 2025年7月18日       | us-east1                   | 约2小时           | 多产品错误率升高                                                                  | 未详细披露                                        | 中型故障，恢复较快                                  |

## Azure的故障

Microsoft Azure（微软云平台）从2014年到2025年这段时间内，**真正影响面广、持续时间长、社会关注度高的重大事故**数量不算特别多（相比早期体量较小时的频繁小故障已显著减少），但几次事件确实造成了全球性或多服务级别的严重影响，尤其涉及**Microsoft 365、Teams、Xbox、Outlook**等消费级/企业级产品时，传播效应非常明显。

下面按时间顺序列出公认的**重大级别故障**（基于Azure官方Status History、Post Incident Reviews、媒体报道、Wikipedia和行业复盘等公开信息）：

| 时间                | 地域/范围                  | 持续时长          | 主要影响范围与后果                                                                 | 主流原因说明                                      | 备注 / 行业评价                                      |
|---------------------|----------------------------|-------------------|------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------|
| 2014年8月14–18日    | US Central、US East、US East 2、Europe North | 多日多次，单次数小时 | Cloud Services、SQL Database、VM、Websites、HDInsight、Mobile Services、Service Bus等大面积不可用 | 多起网络/存储问题叠加                             | 2014年最集中的一波故障，当时Azure还较年轻           |
| 2014年11月18–19日   | **多地域**（美、欧、亚部分） | 约11小时          | Azure Storage为核心，引发VM、Websites、Visual Studio Online、Xbox Live、MSN、Search等20+服务中断 | 存储性能优化配置变更导致Blob前端无限循环          | **Azure早期最严重的一次**，官方详细RCA，赔付客户   |
| 2016年9月15日       | **全球**                   | 数小时            | DNS解析大面积故障，影响大量依赖Azure DNS的服务                                    | 全球DNS问题                                       | 暴露DNS单点风险                                      |
| 2018年6月20日       | 北美多个数据中心           | 数小时–1天+       | 冷却系统故障（雷击+浪涌保护不足）导致多服务中断                                    | 物理设施问题（雷击引发连锁反应）                  | 罕见的硬件+基础设施类大故障                         |
| 2018年9月4日        | **多地域**                 | 超过25小时（部分服务3天） | 多项核心服务长时间不可用                                                          | 冷却系统故障（雷击+浪涌保护不足）                  | 恢复时间最长的一次之一                               |
| 2023年1月23日       | **全球**（核心网络受影响） | 约3小时           | Microsoft 365（Teams、Outlook、Exchange）、部分Azure服务中断                       | 广域网（WAN）问题                                 | M365集体“崩”，影响巨大                               |
| 2024年7月18日       | US Central                 | 约半天            | Virtual Machines等服务管理操作失败，客户无法访问托管服务                          | 访问控制错误 + 基础设施故障                      | 与次日CrowdStrike全球蓝屏事件时间接近，但独立       |
| 2025年1月8–9日      | East US 2 等               | 数小时            | Azure Databricks、Synapse、Functions、App Service、VM等网络中断                    | 网络组件问题                                      | 2025年初较显著的一次                                 |
| 2025年10月29日      | **全球**                   | 约8小时           | Azure Front Door为核心，引发Microsoft 365、Outlook、Teams、Xbox Live、Minecraft、Copilot大面积瘫痪；第三方如Alaska Airlines、Heathrow机场、Costco、Starbucks等受波及 | **Azure Front Door配置变更** + 防护机制bug导致配置不一致全局传播 | **2025年Azure最严重的一次**，Downdetector超3万报告，类似AWS同月故障 |
| 2025年11月5–6日     | West Europe (AZ01)         | 约9–10小时        | VM、PostgreSQL/MySQL Flexible Server、AKS、Storage、Service Bus等多服务降级/中断   | 数据中心热事件（Thermal event）                   | 区域级较严重故障                                     |

### 重要观察与趋势（2014–2025）

- **2014年**：Azure还处于快速扩张期，**配置变更**和**存储层**问题频发，是故障最集中的一年（尤其是11月那次被视为经典案例）。
- **2015–2019**：故障频率下降，但仍以**单地域**或**基础设施**（冷却、雷击、DNS）为主，影响面相对可控。
- **2020–2023**：重大全局故障较少，更多是**网络**或**M365依赖Azure**导致的间接影响（如2023年1月）。
- **2024–2025**：控制面/边缘服务（如**Azure Front Door**）成为新痛点，2025年10月29日事件被公认为近几年Azure最严重的**全球性中断**，影响范围堪比阿里云2023年11月或谷歌云2025年6月的事件。
- **典型特点**：
  - 微软每次重大事故后都会发布详细**Post Incident Review (PIR)**（官方复盘），透明度较高。
  - 经常因**配置变更**、**控制平面组件**、**网络**问题引发（而非底层硬件故障）。
  - 第三方传播效应极强：一旦M365、Xbox、Teams出问题，社会关注度瞬间拉满。
  - 按SLA赔付（信用额度形式），但客户更在意业务连续性。

总体而言，Azure在2014–2025这12年间真正达到**重大级别（全球/多服务长时间中断）**的故障大约**8–10次**，严重程度和频率与AWS、GCP处于同一量级，但**配置错误导致的级联故障**是Azure历史上反复出现的模式。


## Cloudflare的故障

Cloudflare（Cloudflare）作为全球最大的CDN、安全、DNS和边缘计算提供商之一，从2014年到2025年期间，**真正影响面广、造成互联网大面积瘫痪的重大事故**并不算特别多，但一旦发生，通常会波及**数百万到数亿用户**，影响范围极广（因为Cloudflare承载了全球约20–25%的网页流量）。

Cloudflare的故障特点是：**恢复速度通常较快**（大部分在1–4小时内缓解），但**传播效应极强**——一旦核心代理层、DNS或安全组件出问题，很多顶级网站（X、ChatGPT、Shopify、Discord、Spotify、AWS部分服务等）会同时出现5xx错误或无法访问。

下面按时间顺序列出公认的**重大级别故障**（基于Cloudflare官方博客、status.cloudflare.com历史、媒体报道、Wikipedia和行业复盘，重点筛选全球/核心流量中断事件）：

| 时间                | 范围                       | 持续时长          | 主要影响范围与后果                                                                 | 主流原因说明                                      | 备注 / 行业评价                                      |
|---------------------|----------------------------|-------------------|------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------|
| 2019年7月2日        | **全球**                   | 约1–2小时         | 大量网站出现502/503/504错误，互联网大片区域无法访问                               | 软件部署引入严重bug，导致代理层崩溃               | **Cloudflare公认史上最严重的一次**，官方详细复盘   |
| 2020年多次          | 部分地域/控制面            | 数小时            | 仪表盘、分析、部分API不可用；核心代理层基本稳定                                   | 控制平面组件问题                                  | 影响开发者较多，普通用户感知较小                     |
| 2022年6月           | **多数据中心**（19个）     | 约1.5小时         | 核心代理层中断，多个网站无法访问                                                   | 网络配置错误                                      | 中等规模，恢复较快                                   |
| 2025年3月21日       | **全球**                   | 约1小时7分钟      | 存储读写严重故障，影响大量依赖Cloudflare存储/缓存的服务                           | KV/存储层写失败 + 部分读异常                      | 2025年早期较显著的一次                               |
| 2025年6月12日       | **全球**（部分新功能）     | 数小时            | 部分新功能/服务不可用，核心流量基本正常                                           | 特定模块部署问题                                  | 非核心流量中断，影响有限                             |
| 2025年7月14日       | **全球**（1.1.1.1 DNS）    | 约62分钟          | 公共DNS解析器（1.1.1.1）完全不可用，大量用户无法上网                              | 配置错误导致BGP路由撤回，DNS前缀从全球路由表消失   | **对依赖1.1.1.1的用户影响极大**，堪称“断网级”       |
| 2025年10月左右      | 部分服务                   | 数十到几十分钟    | DNS解析短暂中断                                                                   | DNS相关配置问题                                   | 中型故障                                             |
| 2025年11月18日      | **全球**                   | 约4–5小时（高峰期更长） | **大规模互联网中断**：X（Twitter）、ChatGPT、Shopify、Spotify、Letterboxd、Indeed、Canva、Uber、DoorDash、Truth Social、League of Legends等大量顶级服务瘫痪；约20%网页流量受影响，1/3 Alexa前1万网站受波及 | Bot Management特征文件异常膨胀（数据库权限变更导致文件大小翻倍）→ 全网传播 → 代理层崩溃 | **2025年最严重的一次**，也是2019年后Cloudflare最严重的全局流量中断 |
| 2025年12月5日       | **全球**                   | 数小时            | 再次出现大范围5xx错误，Shopify、Zoom、Vinted、Fortnite、Square、Just Eat、Canva、Vimeo、AWS部分服务、Deliveroo等受影响 | 未见完整官方根因（疑似配置/传播类问题）           | **距离11月18日仅17天**，连续两次重大事故引发强烈质疑 |

### 重要观察与趋势（2014–2025）

- **2014–2018早期**：Cloudflare体量快速增长，但公开的**全局重大故障**记录很少，主要是一些局部、地域性或功能性问题。那时互联网对Cloudflare的依赖度远低于现在。
- **2019年7月**：成为Cloudflare历史上最经典的“黑天鹅”事件，此后6年多时间里再未出现同等规模的**核心代理层全局中断**。
- **2025年成为异常年份**：
  - 出现了至少**3–4次**影响面较大的全球/近全球事件（尤其是11月18日和12月5日连续两次）。
  - **11月18日** 被公认为**2019年后最严重的一次**，影响范围和深度都极高。
  - 12月5日再次中断，让很多人质疑Cloudflare的变更控制、回滚机制和“fail small”原则执行情况。
- **典型特点**：
  - 大部分严重故障都与**配置变更**、**特征文件/规则传播**、**控制面组件**或**DNS/BGP**相关。
  - 官方复盘非常透明：每次重大事故后都会在blog.cloudflare.com发布详细post-mortem（根因、时间线、改进措施）。
  - 恢复速度通常较快（回滚 + 停止传播），但影响传播极广（因为Cloudflare的Anycast网络和安全挑战机制）。
  - 不像AWS/Azure/Google那样有明确的SLA信用赔付，但会提供详细解释和改进承诺。

总体而言，Cloudflare在2014–2025这12年间真正达到**重大级别（全球核心流量长时间中断）**的故障大约**5–7次**，其中**2019年7月**和**2025年11月18日**是公认的两个峰值事件。2025年故障频次明显上升，引发了行业对“互联网基础设施集中化风险”的新一轮讨论。


## 腾讯云的故障

腾讯云（Tencent Cloud）从2014年到2025年期间，作为中国第二大公有云厂商（仅次于阿里云），整体稳定性在国内云厂商中属于中上水平，真正**影响面极广、持续时间长、社会关注度高的全局/多地域重大事故**数量相对较少，但一旦出现控制面（管控台/API）或核心存储问题，影响会迅速放大到大量企业和开发者。

腾讯云的**status页面**（https://status.cloud.tencent.com/history）公开透明度相对较低，历史事件列表通常只展示最近1年，且很多中大型故障不会出现在公开列表中，主要靠官方微信公众号、技术博客、媒体报道和社区讨论拼凑完整图景。

下面按时间顺序列出公认的**比较严重的、影响面较广**的腾讯云故障事件（基于官方复盘、媒体报道、知乎/微博/开发者社区讨论等公开信息）：

| 时间                | 地域/范围                  | 持续时长          | 主要影响范围与后果                                                                 | 主流原因说明                                      | 备注 / 行业评价                                      |
|---------------------|----------------------------|-------------------|------------------------------------------------------------------------------------|---------------------------------------------------|-----------------------------------------------------|
| 2014年11月2日       | **全国**（管控面+部分服务）| 约6分钟           | 腾讯云官网访问慢、图片加载失败、控制台异常，部分用户无法正常使用                  | 未详细公开（疑似网络/负载问题）                   | 早期小体量时期，影响有限，但被当时媒体广泛报道       |
| 2018年8月           | 部分用户/云硬盘            | 不确定（单用户影响数小时至永久） | 多位用户云服务器磁盘数据**清零/丢失**，涉及千万元级别损失                         | 磁盘静默错误 + 数据迁移过程中校验/副本机制失效    | **腾讯云历史上最严重的“丢数据”事件**，引发云安全信任危机，官方发详细复盘 |
| 2023年（零星报道）  | 部分地域/服务              | 数十分钟–数小时   | 零星控制台/API异常、存储访问抖动                                                   | 未见公开详细复盘                                  | 相比阿里云同年11月全球事故，腾讯云这一年相对平稳     |
| 2024年4月8日        | **全球17个地域**           | 约74–87分钟       | 控制台完全无法登录、云API全面报错（504 Gateway Timeout）、CVM/RDS等实例正常运行但无法管理/续费/扩容等操作；1957个客户报障 | 云API新版本向前兼容性不足 + 配置数据灰度机制缺失 → 全量发布导致全局传播 | **腾讯云近年来最严重的一次**，被广泛称为“全球大故障”“管控面崩盘”，类似阿里云2023年11月风格 |
| 2025年10月15日      | 多地域                     | 约数十分钟–1小时  | 弹性伸缩（Auto Scaling）等服务异常                                                    | 未详细公开                                        | 来自status页面，属于中型故障                         |
| 2025年10月17日      | 广州地域                   | 约1小时多         | 智能数智人相关服务异常                                                             | 未详细公开                                        | 区域级，影响特定AI/数字人产品                        |

### 重要观察与趋势（2014–2025）

- **早期（2014–2018）**：故障多为**存储层数据丢失**或**短时访问异常**，典型如2018年“丢数据”事件对企业信任打击最大。
- **2019–2023**：腾讯云故障频次和严重度明显下降，鲜有全国/全球级事件，稳定性优于阿里云同期（尤其是2023年阿里云11·12史诗级故障时腾讯云表现平稳）。
- **2024–2025**：2024年4月8日成为转折点，这次**管控面全局故障**让很多人重新审视腾讯云的“变更安全”和“灰度发布”能力。2025年出现几次中型事件，但未见类似2024年4月或阿里云/谷歌云那种“全服务瘫痪”级别。
- **典型特点**：
  - **控制面/API问题**是近年最主要痛点（2024年4月事件典型代表）。
  - **存储/数据丢失**类事故对企业杀伤力最大（2018年案例）。
  - 腾讯云官方复盘相对及时（尤其是重大事件会在公众号、技术社区发详细说明）。
  - 没有像AWS/Azure/Google那样严格的SLA信用赔付机制，但会提供代金券/补偿。
  - 故障传播效应不如阿里云、Cloudflare那么剧烈（因为腾讯云客户结构更偏企业/游戏/视频，对消费互联网依赖相对低）。

总体而言，腾讯云在2014–2025这12年间真正称得上**重大级别（全国/全球管控面长时间不可用或严重数据丢失）**的故障大约**3–5次**，其中**2018年丢数据**和**2024年4月8日全球管控面故障**是公认影响力和讨论度最高的两起。

## 故障透明度

就故障透明度而言，阿里云和腾讯云是比较薄弱的。
阿里云的健康看板（https://status.aliyun.com/#/?region=cn-shanghai）和腾讯云的健康看板（https://status.cloud.tencent.com/history）只显示了过去1年发生过的历史事件。

Azure（https://azure.status.microsoft/en-us/status/history/）保留了5年，cloudflare（https://www.cloudflarestatus.com/history?page=17）最详细透明，可以使用page一直翻页到前面几年的事故。

## 公有云“死因”

在我年轻的时候，我热衷于查阅并重构别人的代码。直到Java Boy给了我上了一课：
即便我帮他修复了内存泄露的问题，他感受到的是一种来自内心深处的存在主义危机。他以愤怒作为掩盖自己无能的藉口，把“维持kubernetes平台稳定”的罪责推到我身上。

其实我也不是很乐意帮他擦屁股。我只是觉得频繁出现的`OOM kill` event 报警让我很烦躁而已。从这个故事中，我领悟了0的哲学：**边际收益为0的重构没人愿意做**。

因此便能推导得出软件行业的**熵增定律**：项目代码库会随着时间推移而变成一座屎山。

有人问，这跟公有云的死因有什么关系呢？

实际上是非常有关系的。因为“工业克苏鲁”要求企业贡献随时间递增的利润。因此对于公有云厂商，对于他们的基层员工而言，就必须“讲一种新的增长故事”，比如现在是2026年，就以AI作为新的增长极。

而旧的的代码就一直烂在那里。因为修改他们没有任何边际收益。

于是大家就逐渐形成这种共识：不修改别人的代码，任由它成为一座屎山。代码如此，架构亦如此。即便网络拓扑成为纯粹的相互依赖的网状结构，反正出事的时候大家都用不了，当宕机责任被平等分摊，自己便没有责任。

讽刺的是，一个在职期间0故障发生，天天上班睡觉的运维工程师，会被认为是一个不称职的人。因为他看起来啥事都没做。然而反直觉的是，这种人其实应该当成企业的吉祥物供起来，因为你根本不知道他之前为了公司的业务稳定付出了多少努力。或者说他纯粹是命好，配享大庙。

实际上，最好的运维在于不运维。因为**运维收入与职业风险完全不对等**。上级不会因为你删除了一个看起来没用的配置，而在这个月多给你发一笔补贴；但你会因为删除了一个有用的数据库配置，而遭到客户的辱骂。

公有云企业忘了，网络效应可以把他们扶起来，也可以把他们重重地摔下去。当他们的客户越多，单次宕机而产生的网络连锁反应就会越大。就好比2025年12月4日晚约21:00–23:37的支付宝故障，这已经是阿里系在2025年的第六次大故障。

当用户规模以亿作为计算单位，每一秒都有人在支付。然而业务又要求往软件上继续加新的功能，最后整个系统不堪重负。

而且公有云服务厂商赔付完全不对等。像支付宝这种最多就是多退少补，少收用户的钱当做福利。但政企的损失则完全无法估算。如果业务寄宿在公有云上，而公有云宕机了，那么作为业务方的你，要怎么向公有云介绍自己的损失呢？

“我的系统每天交易几百个亿，你赔我几个亿？”

客户的损失无法量化，因此阿里云通常都是赔点代金券。但这完全是杯水车薪。损失的时间，谁也无法说清楚实际的业务影响和时间价值。

网络效应带来了公有云指数级别的收入增长。但作为大型政企用户而言，对公有云反依赖应该提上日程。把自己的命运跟单一的云厂商绑定，无法应对突发的风险。

## 小并发高可用系统

在此基础上，我提出我的“小并发高可用系统”概念。我的初步设想是通过存储的冗余，实现业务的高可用。
当流量平稳分摊到设计冗余的信息系统，既规避了集中式流量带来的单集群流量洪峰，又减少了问题扩散的半径。

举个最简单的例子。在DNS解析的时候，把广东的DNS解析设置在一个华南区域的阿里云、腾讯云kubernetes集群。每一个集群互不依赖，运行内部完整的信息业务系统。
这样最坏情况，公有云自身出问题，只会出现50%的不可用。

同时不可用，概率极小。

## 孤帆远影碧山尽,唯见长江天际流

## 参考链接

【1】
2025 年 11 月 18 日 Cloudflare 服务中断
https://blog.cloudflare.com/zh-cn/18-november-2025-outage/

【2】
从 AWS 故障看 DNS 的隐形杀伤力：DeepFlow 如何在混乱中快速锁定根因
https://my.oschina.net/u/3681970/blog/18697034

【3】
2023-11-12阿里云故障复盘与分析
https://github.tiankonguse.com/blog/2023/11/29/aliyun-break.html